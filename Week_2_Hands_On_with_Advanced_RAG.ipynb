{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUVRKQtYlM/2/B/O+WK6dY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrathyushaRagavAdari/CS5588_DS_Capstone/blob/main/Week_2_Hands_On_with_Advanced_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm69WrNS0mpm",
        "outputId": "ccf06b43-bdb1-4f94-9c57-d7b51c5c12cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.4)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.7)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (1.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.45)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.13.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.12.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.4)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.13.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.6)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.5.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community faiss-cpu sentence-transformers openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy Policy Document (The \"RAG\" part)\n",
        "policy_text = \"\"\"\n",
        "Security Policy 101:\n",
        "1. Low Risk (Score < 30): Verify using email confirmation only.\n",
        "2. Medium Risk (Score 30-70): Challenge user with a question about their last transaction date.\n",
        "3. High Risk (Score > 70): Challenge user to provide the exact amount of their last purchase and their registered pet's name.\n",
        "4. Flagged Accounts: Immediately suspend and refer to human agent.\n",
        "\"\"\"\n",
        "\n",
        "# Create a dummy User Database (The \"Context\" part)\n",
        "user_db = {\n",
        "    \"john_doe\": {\n",
        "        \"risk_score\": 75,\n",
        "        \"last_transaction\": \"2023-10-12\",\n",
        "        \"last_amount\": \"$45.50\",\n",
        "        \"pet_name\": \"Rover\"\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "QqQrfjsg1JGP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-huggingface faiss-cpu sentence-transformers\n",
        "\n",
        "import os\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "policy_text = \"\"\"\n",
        "VERIFICATION POLICY MANUAL - REV 2026\n",
        "-------------------------------------\n",
        "1. LOW RISK (Score 0-30):\n",
        "   - Protocol: \"Passive Verification\"\n",
        "   - Action: Send a 6-digit code to the registered email. Do not interrupt user flow.\n",
        "\n",
        "2. MEDIUM RISK (Score 31-70):\n",
        "   - Protocol: \"Knowledge Challenge\"\n",
        "   - Action: Ask the user to verify the date of their last transaction.\n",
        "   - Governance: If the user gets it wrong once, lock account for 1 hour.\n",
        "\n",
        "3. HIGH RISK (Score 71-99):\n",
        "   - Protocol: \"Biometric & Knowledge Depth\"\n",
        "   - Action: Require TWO factors:\n",
        "     1. The exact dollar amount of the last transaction.\n",
        "     2. The name of the registered pet.\n",
        "   - Governance: Any failure results in immediate account suspension.\n",
        "\n",
        "4. CRITICAL/UNKNOWN (Score 100+):\n",
        "   - Protocol: \"Human Handoff\"\n",
        "   - Action: Do not attempt automated verification. Output \"MANUAL_REVIEW_REQUIRED\".\n",
        "\"\"\"\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "docs = text_splitter.create_documents([policy_text])\n",
        "\n",
        "print(\"Building Vector Database... (This runs locally on Colab CPU)\")\n",
        "# 'all-MiniLM-L6-v2' is a small, fast model that downloads once\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vector_db = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "print(\"‚úÖ Vector Database Built Successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9x5yMbCI1V0P",
        "outputId": "7c3ce751-7268-4619-dd5c-f6de5ebbbe1b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.36.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (1.2.7)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain-huggingface) (0.22.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2025.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.32.5)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.6.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.12.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0.0,>=0.33.4->langchain-huggingface) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-huggingface) (0.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building Vector Database... (This runs locally on Colab CPU)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vector Database Built Successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes langchain-huggingface\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter\n",
        "\n",
        "print(\"üîÑ Loading Local 'Flan-T5-Large' Model... (This uses Colab RAM, not API)\")\n",
        "\n",
        "model_id = \"google/flan-t5-large\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    temperature=0.1,\n",
        "    do_sample=True\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "print(f\"‚úÖ Success: {model_id} is loaded and ready.\")\n",
        "\n",
        "template = \"\"\"\n",
        "You are a Security Agent. strictly follow the policy below.\n",
        "\n",
        "POLICY:\n",
        "{policy_context}\n",
        "\n",
        "CURRENT USER STATUS:\n",
        "Risk Score: {risk_score}\n",
        "User Profile: {user_profile}\n",
        "\n",
        "INSTRUCTIONS:\n",
        "1. Check the policy for the given risk score.\n",
        "2. Write the specific verification challenge question for this user.\n",
        "3. If the policy says manual review, write exactly: \"MANUAL_REVIEW_REQUIRED\".\n",
        "\n",
        "YOUR RESPONSE:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"policy_context\": itemgetter(\"risk_query\") | retriever | format_docs,\n",
        "        \"user_profile\": itemgetter(\"user_profile\"),\n",
        "        \"risk_score\": itemgetter(\"risk_score\")\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "current_user_context = {\n",
        "    \"username\": \"j_doe\",\n",
        "    \"risk_score\": 75,\n",
        "    \"last_transaction_date\": \"2025-10-12\",\n",
        "    \"last_amount\": \"$45.50\",\n",
        "    \"pet_name\": \"Rover\"\n",
        "}\n",
        "\n",
        "print(f\"ü§ñ Processing User: {current_user_context['username']} (Risk: {current_user_context['risk_score']})\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"risk_query\": f\"What is the protocol for risk score {current_user_context['risk_score']}?\",\n",
        "    \"user_profile\": str(current_user_context),\n",
        "    \"risk_score\": str(current_user_context['risk_score'])\n",
        "})\n",
        "\n",
        "print(\"Agent Output:\\n\" + response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yd6Dr3A-FkL",
        "outputId": "db333a5e-38cb-41e7-f3ec-14af3c22e3b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading Local 'Flan-T5-Large' Model... (This uses Colab RAM, not API)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success: google/flan-t5-large is loaded and ready.\n",
            "ü§ñ Processing User: j_doe (Risk: 75)\n",
            "----------------------------------------\n",
            "Agent Output:\n",
            "'username': 'j_doe', 'risk_score': 75, 'last_transaction_date': '2025-10-12', 'last_amount': '$45.50', 'pet_name': 'Rover'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GOVERNANCE LAYER (The Safety Guardrail)\n",
        "\n",
        "def secure_identity_gateway(user_context):\n",
        "    \"\"\"\n",
        "    This function acts as the 'Governance Layer'.\n",
        "    It intercepts the AI's response before the user sees it.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Run the Agent\n",
        "    # We construct the query dynamically based on risk score\n",
        "    query_text = f\"What is the protocol for risk score {user_context['risk_score']}?\"\n",
        "\n",
        "    raw_response = chain.invoke({\n",
        "        \"risk_query\": query_text,\n",
        "        \"user_profile\": str(user_context),\n",
        "        \"risk_score\": str(user_context['risk_score'])\n",
        "    })\n",
        "\n",
        "    # 2. The \"Safety Check\" (Governance)\n",
        "    # If the model flags a critical risk, we override the output to a system lock.\n",
        "    if \"MANUAL_REVIEW_REQUIRED\" in raw_response:\n",
        "        return {\n",
        "            \"status\": \"üî¥ BLOCKED\",\n",
        "            \"action\": \"ACCOUNT LOCKED. Ticket #9928 created for Human Review.\",\n",
        "            \"raw_log\": raw_response\n",
        "        }\n",
        "\n",
        "    # 3. If safe, pass the challenge through\n",
        "    return {\n",
        "        \"status\": \"üü¢ ACTIVE\",\n",
        "        \"action\": f\"Challenge Issued: {raw_response}\",\n",
        "        \"raw_log\": raw_response\n",
        "    }\n",
        "\n",
        "# --- TEST THE GOVERNANCE LAYER ---\n",
        "\n",
        "# Case A: Normal User (Risk 75)\n",
        "print(\"Testing Normal User...\")\n",
        "user_safe = {\"username\": \"alice\", \"risk_score\": 75, \"user_profile\": \"Standard User\"}\n",
        "print(secure_identity_gateway(user_safe))\n",
        "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
        "\n",
        "# Case B: Critical User (Risk 150) -> Should Trigger Lock\n",
        "print(\"Testing Critical Risk User...\")\n",
        "user_risky = {\"username\": \"hacker_bot\", \"risk_score\": 150, \"user_profile\": \"Unknown Device\"}\n",
        "print(secure_identity_gateway(user_risky))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uE0B9ne_FxN",
        "outputId": "8af80d9d-5deb-4ad4-8d09-dd6c74689021"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Normal User...\n",
            "{'status': 'üü¢ ACTIVE', 'action': \"Challenge Issued: 'username': 'alice', 'risk_score': 75, 'user_profile': 'Standard User'\", 'raw_log': \"'username': 'alice', 'risk_score': 75, 'user_profile': 'Standard User'\"}\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Testing Critical Risk User...\n",
            "{'status': 'üü¢ ACTIVE', 'action': \"Challenge Issued: 'username': 'hacker_bot', 'risk_score': 150, 'user_profile': 'Unknown Device'\", 'raw_log': \"'username': 'hacker_bot', 'risk_score': 150, 'user_profile': 'Unknown Device'\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FAILURE SIMULATION & SYSTEM FIX\n",
        "\n",
        "# 1. THE FAILURE\n",
        "# Scenario: A new threat type \"Deepfake\" appears, but it's not in our policy text.\n",
        "print(\"üí• SIMULATING FAILURE: Unknown Threat Type\")\n",
        "unknown_context = {\"username\": \"fake_video\", \"risk_score\": 90, \"notes\": \"Deepfake Detected\"}\n",
        "\n",
        "# The agent will likely struggle or give the wrong rule because \"Deepfake\" isn't in the DB.\n",
        "response = chain.invoke({\n",
        "    \"risk_query\": \"Protocol for Deepfake Detected?\",\n",
        "    \"user_profile\": str(unknown_context),\n",
        "    \"risk_score\": \"90\"\n",
        "})\n",
        "print(f\"Agent Output (Likely Wrong/Generic): {response}\")\n",
        "\n",
        "\n",
        "# 2. THE SYSTEM FIX (Data Update)\n",
        "# We update the policy to explicitly handle this new threat.\n",
        "print(\"\\nüõ†Ô∏è APPLYING FIX: Updating Policy & Re-indexing...\")\n",
        "\n",
        "new_policy_rule = \"\"\"\n",
        "5. DEEPFAKE THREAT (Any Score):\n",
        "   - Protocol: \"Liveness Failure\"\n",
        "   - Action: Do not challenge. Auto-ban user UUID immediately.\n",
        "\"\"\"\n",
        "\n",
        "# Append and Re-process\n",
        "updated_policy = policy_text + \"\\n\" + new_policy_rule\n",
        "new_docs = text_splitter.create_documents([updated_policy])\n",
        "vector_db = FAISS.from_documents(new_docs, embeddings) # Rebuild the brain\n",
        "retriever = vector_db.as_retriever(search_kwargs={\"k\": 1})\n",
        "\n",
        "# Re-build the chain with the new retriever\n",
        "chain = (\n",
        "    {\n",
        "        \"policy_context\": itemgetter(\"risk_query\") | retriever | format_docs,\n",
        "        \"user_profile\": itemgetter(\"user_profile\"),\n",
        "        \"risk_score\": itemgetter(\"risk_score\")\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 3. VERIFY THE FIX\n",
        "print(\"\\n‚úÖ VERIFYING FIX:\")\n",
        "response_fixed = chain.invoke({\n",
        "    \"risk_query\": \"Protocol for Deepfake Detected?\",\n",
        "    \"user_profile\": str(unknown_context),\n",
        "    \"risk_score\": \"90\"\n",
        "})\n",
        "print(f\"New Agent Output: {response_fixed}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyLeYtGH_QqP",
        "outputId": "b827aa33-037d-48ab-9175-b7d459a693d5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üí• SIMULATING FAILURE: Unknown Threat Type\n",
            "Agent Output (Likely Wrong/Generic): 'username': 'fake_video', 'risk_score': 90, 'notes': 'Deepfake Detected'\n",
            "\n",
            "üõ†Ô∏è APPLYING FIX: Updating Policy & Re-indexing...\n",
            "\n",
            "‚úÖ VERIFYING FIX:\n",
            "New Agent Output: 'username': 'fake_video', 'risk_score': 90, 'notes': 'Deepfake Detected'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q rank_bm25 langchain langchain-community\n",
        "\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from langchain.retrievers import EnsembleRetriever\n",
        "    print(\"‚úÖ Loaded EnsembleRetriever from langchain package.\")\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è Import failed. Using local 'EnsembleRetriever' fallback to avoid restart.\")\n",
        "    from langchain_core.retrievers import BaseRetriever\n",
        "    from langchain_core.documents import Document\n",
        "    from typing import List\n",
        "\n",
        "    class EnsembleRetriever(BaseRetriever):\n",
        "        retrievers: List[BaseRetriever]\n",
        "        weights: List[float]\n",
        "\n",
        "        def _get_relevant_documents(self, query: str, *, run_manager=None) -> List[Document]:\n",
        "            all_docs = []\n",
        "            for r in self.retrievers:\n",
        "                all_docs.extend(r.invoke(query))\n",
        "            seen = set()\n",
        "            unique_docs = []\n",
        "            for d in all_docs:\n",
        "                if d.page_content not in seen:\n",
        "                    unique_docs.append(d)\n",
        "                    seen.add(d.page_content)\n",
        "            return unique_docs[:4]\n",
        "# --- END FIX ---\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully.\")\n",
        "\n",
        "# 1. SETUP THE TARGET DOCUMENTS\n",
        "if 'new_docs' in globals():\n",
        "    print(\"‚ÑπÔ∏è Using Updated Policy Documents (Deepfake rule included).\")\n",
        "    target_docs = new_docs\n",
        "elif 'docs' in globals():\n",
        "    print(\"‚ö†Ô∏è Using Original Policy Documents (Deepfake rule might be missing).\")\n",
        "    target_docs = docs\n",
        "else:\n",
        "    raise ValueError(\"‚ùå No documents found! Please run the 'Vector Database' cell first.\")\n",
        "\n",
        "# 2. RE-INITIALIZE EMBEDDINGS\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# 3. BUILD VECTOR RETRIEVER\n",
        "print(\"1Ô∏è‚É£ Building Vector Retriever...\")\n",
        "vector_db = FAISS.from_documents(target_docs, embeddings)\n",
        "vector_retriever = vector_db.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# 4. BUILD KEYWORD RETRIEVER\n",
        "print(\"2Ô∏è‚É£ Building Keyword Retriever (BM25)...\")\n",
        "keyword_retriever = BM25Retriever.from_documents(target_docs)\n",
        "keyword_retriever.k = 2\n",
        "\n",
        "# 5. BUILD HYBRID RETRIEVER\n",
        "print(\"3Ô∏è‚É£ Assembling Hybrid Retriever (Real Weighted Version)...\")\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=[vector_retriever, keyword_retriever],\n",
        "    weights=[0.5, 0.5]\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Success: Hybrid Retriever is ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-qdzlpdDeYq",
        "outputId": "8b1ac2e1-d068-40f3-e7f9-722439b03cc0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è Import failed. Using local 'EnsembleRetriever' fallback to avoid restart.\n",
            "‚úÖ Libraries imported successfully.\n",
            "‚ÑπÔ∏è Using Updated Policy Documents (Deepfake rule included).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3650594660.py:50: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1Ô∏è‚É£ Building Vector Retriever...\n",
            "2Ô∏è‚É£ Building Keyword Retriever (BM25)...\n",
            "3Ô∏è‚É£ Assembling Hybrid Retriever (Real Weighted Version)...\n",
            "‚úÖ Success: Hybrid Retriever is ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATION METRICS (Precision & Trust)\n",
        "\n",
        "def evaluate_system():\n",
        "    test_set = [\n",
        "        {\"query\": \"Score 20\", \"expected\": \"Passive Verification\"},\n",
        "        {\"query\": \"Score 75\", \"expected\": \"Biometric\"},\n",
        "        {\"query\": \"Score 150\", \"expected\": \"MANUAL_REVIEW\"}\n",
        "    ]\n",
        "\n",
        "    correct_retrievals = 0\n",
        "    total_trust_score = 0\n",
        "\n",
        "    print(\"\\nüìä RUNNING SYSTEM EVALUATION...\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for case in test_set:\n",
        "        retrieved_docs = ensemble_retriever.invoke(case[\"query\"])\n",
        "        retrieved_text = retrieved_docs[0].page_content\n",
        "        print(f\"üîç Query: '{case['query']}'\")\n",
        "        if case[\"expected\"] in retrieved_text:\n",
        "            correct_retrievals += 1\n",
        "            print(f\"‚úÖ Query '{case['query']}' -> Found correct policy.\")\n",
        "            total_trust_score += 5 # 5/5 Trust\n",
        "        else:\n",
        "            print(f\"‚ùå Query '{case['query']}' -> Missed context.\")\n",
        "            total_trust_score += 1 # 1/5 Trust\n",
        "\n",
        "    precision = correct_retrievals / len(test_set)\n",
        "    avg_trust = total_trust_score / len(test_set)\n",
        "\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"üìà TECHNICAL METRIC: Precision@1 = {precision:.2f}\")\n",
        "    print(f\"ü§ù PRODUCT METRIC: Avg Trust Score = {avg_trust:.1f}/5.0\")\n",
        "\n",
        "    return {\"precision\": precision, \"trust_score\": avg_trust}\n",
        "\n",
        "metrics = evaluate_system()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BTxJ70NDps7",
        "outputId": "30cc9e7d-d018-4a8a-f5b8-e4f750e85a90"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä RUNNING SYSTEM EVALUATION...\n",
            "----------------------------------------\n",
            "üîç Query: 'Score 20'\n",
            "‚ùå Query 'Score 20' -> Missed context.\n",
            "üîç Query: 'Score 75'\n",
            "‚úÖ Query 'Score 75' -> Found correct policy.\n",
            "üîç Query: 'Score 150'\n",
            "‚úÖ Query 'Score 150' -> Found correct policy.\n",
            "----------------------------------------\n",
            "üìà TECHNICAL METRIC: Precision@1 = 0.67\n",
            "ü§ù PRODUCT METRIC: Avg Trust Score = 3.7/5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE AGENT TO USE HYBRID RETRIEVER\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        \"policy_context\": itemgetter(\"risk_query\") | ensemble_retriever | format_docs,\n",
        "        \"user_profile\": itemgetter(\"user_profile\"),\n",
        "        \"risk_score\": itemgetter(\"risk_score\")\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Agent is now connected to the Hybrid Brain (BM25 + Vectors).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9-ARkTOJOyc",
        "outputId": "8f5a7e64-0ec1-4228-973c-388576f7e21d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Agent is now connected to the Hybrid Brain (BM25 + Vectors).\n"
          ]
        }
      ]
    }
  ]
}